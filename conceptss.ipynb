{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbrIctw2H6SMycwJUpSZF2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kirtanaaa/ML_Classification/blob/main/conceptss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## False Positives and False Negatives"
      ],
      "metadata": {
        "id": "TCFyRB21LVMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "False Positives and False Negatives are terms used to describe errors made by a classification model when predicting binary outcomes (e.g., yes/no, 1/0) or when evaluating the performance of the model.\n",
        "\n",
        "False Positive (FP):\n",
        "\n",
        "1. A False Positive occurs when the model predicts a positive outcome (e.g., \"Yes\" or \"1\") for a sample that actually belongs to the negative class (e.g., \"No\" or \"0\").\n",
        "\n",
        "2. In other words, the model wrongly identifies something as positive when it should have been negative.\n",
        "\n",
        "3. For example, in a medical diagnosis scenario, a false positive would be when the model predicts that a person has a disease when they are actually healthy.\n",
        "\n",
        "False Negative (FN):\n",
        "\n",
        "1. A False Negative occurs when the model predicts a negative outcome (e.g., \"No\" or \"0\") for a sample that actually belongs to the positive class (e.g., \"Yes\" or \"1\").\n",
        "\n",
        "2. In other words, the model wrongly identifies something as negative when it should have been positive.\n",
        "\n",
        "3. For example, in a spam email detection system, a false negative would be when the model fails to identify a spam email and classifies it as legitimate.\n",
        "\n",
        "In the context of evaluating a classification model, False Positives and False Negatives are important metrics to consider along with True Positives (correctly predicted positive samples) and True Negatives (correctly predicted negative samples).\n",
        "\n",
        "They are typically used to calculate performance metrics like precision, recall, F1 score, and the confusion matrix."
      ],
      "metadata": {
        "id": "dDXDbfcjLeeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "                   | Actual Positive       | Actual Negative\n",
        "-----------------------------------------------------------------\n",
        "Predicted Positive | True Positive  (1,1)  | False Positive (0,1)\n",
        "                   |                       |\n",
        "Predicted Negative | False Negative (1,0)  | True Negative  (0,0)\n",
        "\n",
        "\n",
        "#(data says, we say)\n"
      ],
      "metadata": {
        "id": "ayeJ1Po4NT2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix"
      ],
      "metadata": {
        "id": "fi3HalBoNIjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confusion matrix is a table used to evaluate the performance of a classification model on a set of test data for which the true labels are known.\n",
        "\n",
        "It compares the predicted classes with the actual classes and provides a clear view of how well the model is performing in terms of true positive, false positive, true negative, and false negative predictions.\n",
        "\n",
        "The confusion matrix allows us to understand the types of errors the model is making and to choose appropriate evaluation metrics based on the problem's requirements.\n",
        "\n",
        "It provides valuable insights into the model's strengths and weaknesses and helps in fine-tuning the model for better performance."
      ],
      "metadata": {
        "id": "tm8OeDUXOgBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "                   | Actual Positive       | Actual Negative\n",
        "-----------------------------------------------------------------\n",
        "Predicted Positive | True Positive  (1,1)  | False Positive (0,1)\n",
        "                   |                       |\n",
        "Predicted Negative | False Negative (1,0)  | True Negative  (0,0)\n",
        "\n",
        "\n",
        "#(data says, we say)"
      ],
      "metadata": {
        "id": "6M9FkkeEO0ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's what each cell of the confusion matrix represents:\n",
        "\n",
        "1. True Positive (TP): The number of samples that belong to the positive class (e.g., \"Yes\" or \"1\") and were correctly predicted by the model as positive.\n",
        "\n",
        "2. False Positive (FP): The number of samples that belong to the negative class (e.g., \"No\" or \"0\") but were incorrectly predicted by the model as positive.\n",
        "\n",
        "3. True Negative (TN): The number of samples that belong to the negative class and were correctly predicted by the model as negative.\n",
        "\n",
        "4. False Negative (FN): The number of samples that belong to the positive class but were incorrectly predicted by the model as negative."
      ],
      "metadata": {
        "id": "X5tAGQCCO8zJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider an example using a binary classification problem of predicting whether an email is spam or not:\n",
        "\n",
        "Suppose we have 100 emails in our test set:\n",
        "\n",
        "40 of them are spam (positive class) and 60 are not spam (negative class).\n",
        "\n",
        "After applying the classification model, it makes predictions, and the confusion matrix looks like this:"
      ],
      "metadata": {
        "id": "ibDLp8RKPCvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "                   | Actual Spam  | Actual Not Spam\n",
        "----------------------------------------------------\n",
        "Predicted Spam     | 30 (TP)      | 5  (FP)\n",
        "Predicted Not Spam | 10 (FN)      | 55 (TN)\n"
      ],
      "metadata": {
        "id": "SYQP90-mPIKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy:\n",
        "\n",
        "Accuracy measures the overall correctness of the models predictions:\n",
        "\n",
        "Accuracy = (TP + TN) / Total samples\n",
        "         = (30 + 55) / 100\n",
        "         = 85 / 100\n",
        "         = 0.85"
      ],
      "metadata": {
        "id": "s5K7bZAbPef1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Precision:\n",
        "\n",
        "Precision measures the proportion of correctly predicted positive samples among all predicted positive samples:\n",
        "\n",
        "Precision = TP / (TP + FP)\n",
        "          = 30 / (30 + 5)\n",
        "          = 30 / 35\n",
        "          = 0.8571"
      ],
      "metadata": {
        "id": "gpJXWWxyPl5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Recall (Sensitivity or True Positive Rate):\n",
        "\n",
        "Recall measures the proportion of correctly predicted positive samples among all actual positive samples:\n",
        "\n",
        "Recall = TP / (TP + FN)\n",
        "       = 30 / (30 + 10)\n",
        "       = 30 / 40\n",
        "       = 0.75"
      ],
      "metadata": {
        "id": "nxAGYkkDPtm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Specificity (True Negative Rate):\n",
        "\n",
        "Specificity measures the proportion of correctly predicted negative samples among all actual negative samples:\n",
        "\n",
        "Specificity = TN / (TN + FP)\n",
        "            = 55 / (55 + 5)\n",
        "            = 55 / 60\n",
        "            = 0.9167"
      ],
      "metadata": {
        "id": "we2fFY4iPyQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "F1 Score:\n",
        "\n",
        "The F1 score is the harmonic mean of precision and recall and provides a balance between the two metrics:\n",
        "\n",
        "F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "         = 2 * (0.8571 * 0.75) / (0.8571 + 0.75)\n",
        "         = 2 * 0.642825 / 1.6071\n",
        "         = 1.28565 / 1.6071\n",
        "         = 0.8002"
      ],
      "metadata": {
        "id": "TwHkwD8mP2IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy Paradox"
      ],
      "metadata": {
        "id": "-0WYEVeeQjZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy paradox, also known as the accuracy fallacy, is a phenomenon where a high accuracy rate in a classification model can be misleading and does not necessarily imply a good or reliable model.\n",
        "\n",
        "It occurs when a model performs well in terms of overall accuracy but fails to perform well on specific classes or instances that are more critical or important in the context of the problem.\n",
        "\n",
        "The accuracy paradox is especially relevant when dealing with imbalanced datasets, where one class is significantly more prevalent than the other(s).\n",
        "\n",
        "In such cases, a model that simply predicts the majority class for all instances can achieve a high accuracy because it correctly predicts the majority class most of the time.\n",
        "\n",
        "However, such a model is not useful, as it fails to capture the minority class's important patterns.\n",
        "\n",
        "Let's illustrate the accuracy paradox with an example:\n",
        "\n",
        "Suppose we have a dataset to predict whether a rare disease is present in patients (Class 1: Disease Present) or not (Class 0: Disease Absent).\n",
        "\n",
        "The dataset consists of 99% Class 0 instances (Disease Absent) and only 1% Class 1 instances (Disease Present).\n",
        "\n",
        "If we build a naive model that always predicts Class 0 (Disease Absent) for all instances, it will achieve an accuracy of 99% because it correctly predicts the majority class most of the time.\n",
        "\n",
        "However, the model completely fails to identify the critical cases where the disease is present (Class 1), which is the primary objective of the classification task.\n",
        "\n",
        "In this case, the high accuracy of 99% is misleading and does not represent the model's actual performance.\n",
        "\n",
        "The model's inability to correctly identify the rare but important Class 1 instances makes it practically useless for its intended purpose.\n",
        "\n",
        "To overcome the accuracy paradox, it is essential to consider other performance metrics that provide a more comprehensive evaluation of the model's capabilities, especially when dealing with imbalanced datasets.\n",
        "\n",
        "These metrics include precision, recall, F1 score, and area under the receiver operating characteristic curve (AUC-ROC).\n",
        "\n",
        "These metrics provide a better understanding of the model's performance, particularly in situations where certain classes or instances are more critical or challenging to predict accurately."
      ],
      "metadata": {
        "id": "AQcpxtAeQldz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CAP Curve"
      ],
      "metadata": {
        "id": "GElhLws0Q7zO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Cumulative Accuracy Profile (CAP) curve is a graphical representation that evaluates the performance of a binary classification model, especially in the context of marketing and business applications.\n",
        "\n",
        "It helps to visualize how well the model performs in identifying positive instances (e.g., buyers, responders) within a given portion of the dataset."
      ],
      "metadata": {
        "id": "3Bc7YhVeSbQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A perfect CAP curve would have a steep rise from 0% to 100% on the y-axis, indicating that the model has correctly identified all the positive instances very early in the ranked list. The closer the model's CAP curve is to the perfect curve, the better it performs.\n",
        "\n",
        "Interpreting the CAP Curve:\n",
        "\n",
        "1. The area between the random model curve and the CAP curve indicates the model's effectiveness in identifying positive instances.\n",
        "\n",
        "2. The CAP curve's lift represents how much better the model performs compared to a random model. A higher lift means the model is more effective in identifying positive instances.\n",
        "\n",
        "3. The point at which the CAP curve intersects the perfect model curve shows the percentage of the dataset needed to identify all positive instances if the model performed perfectly.\n",
        "\n",
        "The CAP curve is a valuable tool for assessing the model's performance, especially in scenarios where identifying positive instances is critical, such as targeted marketing campaigns or fraud detection.\n",
        "\n",
        "It helps decision-makers understand the model's effectiveness and make informed decisions based on the model's predictions."
      ],
      "metadata": {
        "id": "4wSdkSulT4bm"
      }
    }
  ]
}